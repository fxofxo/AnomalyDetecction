{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Prepare input Data \n",
    "## split original ECG signal into frames, saving it scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scale = 0.005\n",
       "samplesPerSecond = 360\n",
       "WINDOW = 120\n",
       "WindowsPerFrame = 30\n",
       "STEP = 2\n",
       "SAMPLES = 200000\n",
       "inputDataFile = 105s1.dat\n",
       "samplesPerFrame = 3600\n",
       "secondPerFrame = 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scale = 1/200.0\n",
    "val samplesPerSecond = 360\n",
    "\n",
    "val WINDOW = 120   // samples per window\n",
    "val WindowsPerFrame = 30\n",
    "val STEP = 2\n",
    "val SAMPLES = 200000\n",
    "\n",
    "//val inputDataFile = \"a02.dat\"\n",
    "\n",
    "val inputDataFile = \"105s1.dat\"\n",
    "\n",
    "\n",
    "val samplesPerFrame = WindowsPerFrame * WINDOW\n",
    "val secondPerFrame = samplesPerFrame / samplesPerSecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io._                                                        \n",
    "import scala.collection.mutable.ListBuffer   //bE CAREFULL With java imports, better before scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rootDir = /home/fsainz/data/\n",
       "hdfsOutPath = /user/fsainz/data/out/\n",
       "fileName = /home/fsainz/data/105s1.dat\n",
       "framesFileName = frames-105s1_10sLE.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "frames-105s1_10sLE.csv"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Databricks\n",
    "\n",
    "//dbutils.fs.mkdirs(\"/FileStore/fxo/ted\")\n",
    "//dbutils.fs.ls(\"FileStore\")\n",
    "\n",
    "//val rootDir = \"FileStore/fxo/ted/\"\n",
    "//val fileName = \"/dbfs/\" + rootDir +\"a02.dat\"\n",
    "\n",
    "// GCP\n",
    "\n",
    "val rootDir = \"/home/fsainz/data/\"\n",
    "val hdfsOutPath = \"/user/fsainz/data/out/\"\n",
    "val fileName = rootDir + inputDataFile \n",
    "\n",
    "val framesFileName = \"frames-\"+ inputDataFile.split('.')(0)+\"_\" + secondPerFrame +\"sLE.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating frames of 3600 samples (10s.) long. \n",
      "OUTPUTFILE in hdfs  =/user/fsainz/data/out/frames-105s1_10sLE.csv\n"
     ]
    }
   ],
   "source": [
    "println(\"Generating frames of \" + samplesPerFrame + \" samples (\" + \n",
    "        + secondPerFrame + \"s.) long. \")\n",
    "        println(\"OUTPUTFILE in hdfs  =\" + hdfsOutPath+framesFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file length =1300000\n",
      "nFrames = 180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inFile = /home/fsainz/data/105s1.dat\n",
       "InFileStream = java.io.FileInputStream@3591fdbe\n",
       "input = java.io.DataInputStream@1dc74e06\n",
       "frames = ArrayBuffer(Array(4.675, 4.675, 4.675, 4.675, 4.675, 4.675, 4.675, 4.675, 4.67, 4.69, 4.6850000000000005, 4.715, 4.735, 4.755, 4.765, 4.78, 4.795, 4.815, 4.835, 4.86, 4.875, 4.8950000000000005, 4.88, 4.8950000000000005, 4.91, 4.925, 4.93, 4.925, 4.915, 4.915, 4.92, 4.915, 4.91, 4.905, 4.885, 4.885, 4.9, 4.885, 4.88, 4.865, 4.84, 4.845, 4.855, 4.8500000000000005, 4.8500000000000005, 4.835, 4.815, 4.815, 4.825, 4.83, 4.82, 4.815, 4.805, 4.8100000000000005, 4.805, 4.825, 4.82, 4.8100000000000005, 4.805, 4.805, 4.8, 4.805, 4.815, 4.81000000000...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ArrayBuffer([D@4ec2d06a, [D@576f215f, [D@b6e073c, [D@401fe664, [D@18863f8d, [D@364d801b, [D@4d612609, [D@7272d507, [D@5b21e693, [D@1b6e8b4, [D@4523d7e5, [D@219df8ab, [D@755c8775, [D@43193222, [D@185e495b, [D@5648b25b, [D@47bef0c8, [D@4d3c423f, [D@27c2de52, [D@340b6a9b, [D@403e462, [D@2b017367, [D@1a00081b, [D@7530e672, [D@6e0b3c42, [D@4eb1e0af, [D@54eb79b2, [D@261a062b, [D@11c4d786, [D@50b1e989, [D@5f982cc4, [D@29c55242, [D@1e0ac81b, [D@412887f2, [D@5f487d1f, [D@5c1c73f6, [D@149e327e, [D@118c3008, [D@db017a3, [D@320c4620, [D@57e49fdd, [D@2798e959, [D@2a98ef06, [D@7a759133, [D@91ce3d7, [D@692e018c, [D@4233370, [D@5289d6ce, [D@3ce59513, [D@25dec162, [D@38fbde1a, [D@2458406d, [D@348514f8, [D@670316db, [D@1ee1f2f, [D@67d37c57, [D@7bed9681, [D@427f2e9, [D@75c19225, [D@98c690e, [D@1e42af20, [D@2549b6bb, [D@5c60588d, [D@36ce8e16, [D@3d7e1c40, [D@55ea993a, [D@b10173a, [D@49ec8670, [D@43d1f11, [D@59f65e5, [D@e3d0bab, [D@234fd31a, [D@4fe695f5, [D@31a871a1, [D@572e446c, [D@a248a90, [D@7196ff1c, [D@3c1bdf6, [D@266984c6, [D@837fa91, [D@4e1f8a94, [D@40bfa0ac, [D@2a9a43c9, [D@1dcec779, [D@2f37a57e, [D@12b0caf4, [D@6a4c5760, [D@1d2b05f, [D@13dce94e, [D@72f76ab, [D@f50c679, [D@1b14df84, [D@6ff6d331, [D@13906a4a, [D@1a7a154f, [D@272f017b, [D@7e9921f4, [D@6e4bd657, [D@245166bc, [D@7baa8f94, [D@4c8b9794, [D@7c42193c, [D@5309e48e, [D@7cf00754, [D@6c571869, [D@306726, [D@1c81ce3a, [D@6037a059, [D@34776866, [D@377d2099, [D@259fd852, [D@787791d0, [D@774cf2e0, [D@296b6d78, [D@726d7d4f, [D@36509bc3, [D@269ba41b, [D@54e91d1d, [D@4d7785ca, [D@7fcb9124, [D@40b49ba, [D@1896add3, [D@69be22de, [D@4a5fdc7c, [D@14c71a14, [D@3992d56a, [D@21a2b854, [D@152a3c20, [D@3911932b, [D@160728e6, [D@2253526a, [D@66dca13c, [D@53e1ee07, [D@7c8eac85, [D@44f6717c, [D@47a0ed9f, [D@1bedb2ba, [D@128edfc6, [D@6504217a, [D@2508f450, [D@5306fa29, [D@37927f60, [D@73d98719, [D@11915746, [D@75a220ef, [D@516f3844, [D@6e318c3f, [D@7501cdd8, [D@11c03ecf, [D@39c589ab, [D@5b77bad8, [D@2063d363, [D@3045a7e6, [D@2558f894, [D@431f379d, [D@7dd90c56, [D@60beee9c, [D@2402647, [D@38595a4d, [D@d574aac, [D@1a0a8f94, [D@197bf08c, [D@6b825adc, [D@5c376f2a, [D@128f3d17, [D@5436405a, [D@2d9bb4ba, [D@4a32c01c, [D@21be3bc8, [D@47ec9b35, [D@3722c916, [D@6b4ccdce, [D@3bfccb14, [D@18eaca6d, [D@3921da1f, [D@348e5248, [D@69f79ba6, [D@27009d57, [D@10465a8a, [D@561c56d8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "//val rawData = sc.textFile(\"/FileStore/tables/gqh5mjl51507524821196/a02.dat\", numPartitions)\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "var frames = scala.collection.mutable.ArrayBuffer.empty[Array[Double]]\n",
    "\n",
    "var pos = 0\n",
    "var available = 0\n",
    "\n",
    "\n",
    "val frameBytes =  samplesPerFrame * 2 // read shorts 2 bytes.\n",
    "val nFrames = inFile.length / frameBytes\n",
    "println(\"Input file length =\" + inFile.length  )\n",
    "println(\"nFrames = \" + nFrames)\n",
    "\n",
    "var dataFrame = Array[Double]()\n",
    "for (  w  <- 1 to nFrames.toInt ) {\n",
    "    \n",
    "   dataFrame = new Array[Double](samplesPerFrame)\n",
    "   for ( i <- 0 until samplesPerFrame) {\n",
    "     // read little indiean and scale\n",
    "    val value = java.lang.Short.reverseBytes(input.readShort() )  // two bytes\n",
    "    dataFrame( i ) = (value.toDouble * scale) \n",
    "    \n",
    "  }\n",
    "  frames.append(dataFrame)\n",
    "   \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/fsainz/data/out/frames-105s1_10sLE.csv Written\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvFilePath = /user/fsainz/data/out/frames-105s1_10sLE.csv\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/etc/hive/conf.dist/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-494003913_83, ugi=root (auth:SIMPLE)]]\n",
       "output = org.apache.hadoop.hdfs.client.HdfsDataOutputStream@3830bb88\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "line: String = \"\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.hadoop.hdfs.client.HdfsDataOutputStream@3830bb88"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvFilePath = hdfsOutPath + framesFileName\n",
    "// FileWriter\n",
    "\n",
    "//val file = new File(csvFilePath)\n",
    "//val bw = new BufferedWriter(new FileWriter(file))\n",
    "import org.apache.hadoop.fs.Path;\n",
    "val conf = sc.hadoopConfiguration\n",
    "val fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "val output = fs.create(new Path(csvFilePath))\n",
    "\n",
    "\n",
    "var line = \"\"\n",
    "for ( f <- frames) {\n",
    "  f.foreach( x => line +=  x.toString + \",\" )\n",
    "  output.write((line.substring(0,line.length-1) + \"\\n\").getBytes())\n",
    "  line = \"\"\n",
    " \n",
    "}\n",
    "output.close()\n",
    "println(csvFilePath + \" Written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
