{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Train the model. Gets kmeans dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.version\n",
    "val conf = sc.hadoopConfiguration\n",
    "val fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "val hdfshome = fs.getHomeDirectory\n",
    "println(sc.version)\n",
    "println(hdfshome)\n",
    "val files = fs.listFiles(hdfshome, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rootDir = \"/user/fsainz/data/out\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//fs.exists(new org.apache.hadoop.fs.Path(\"hdfs://spark-clu-m/user/fsainz/data/in/a02.dat\"))\n",
    "fs.exists(new org.apache.hadoop.fs.Path(rootDir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InFile: /home/fsainz/data/105s1.dat\n",
      "Ouput Model: /user/fsainz/data/out/dict-105s1_W120.LEmodel\n"
     ]
    }
   ],
   "source": [
    "import java.io._                                                        \n",
    "import scala.collection.mutable.ListBuffer   //bE CAREFULL With java imports, better before scala.\n",
    "\n",
    "val WINDOW = 120;\n",
    "val STEP = 4;\n",
    "val TRsamples = 200000;\n",
    "val scale = 1/200.0;\n",
    "//val inFileName = \"a02.dat\"     //apnea\n",
    "val inFileName = \"105s1.dat\"   //mit-arritmia\n",
    "\n",
    "val inputPath =  \"/home/fsainz/data/\" //local filesystem\n",
    "val fileName = inputPath + inFileName \n",
    "val hdfsOutPath = \"/user/fsainz/data/out/\"\n",
    "val outFileName = hdfsOutPath + \"dict-\" + inFileName.split('.')(0) + \"_W\" + WINDOW +\".LEmodel\"\n",
    "\n",
    "\n",
    "println(\"InFile: \" + fileName)\n",
    "println(\"Ouput Model: \" + outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nWindows = 1666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "// Now start reading local file   TRaining file is part InFile training and run\n",
    "\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "\n",
    "// val nSamples =  (inFile.length() / 2)   // We are going to read short: 2 bytes..\n",
    "\n",
    "\n",
    "val nWindows = TRsamples / WINDOW\n",
    "println(\"nWindows = \" + nWindows.toString)\n",
    "\n",
    "val buffer = new Array[Double](WINDOW)\n",
    "var windows = scala.collection.mutable.ListBuffer.empty[Array[Double]]\n",
    "\n",
    "var pos = 0\n",
    "var available = 0\n",
    "var dataWindow = new Array[Double](WINDOW)\n",
    "\n",
    "for (  w  <- 1 to nWindows.toInt){\n",
    "    // new object is neeed to append at the end!!\n",
    "  dataWindow = new Array[Double](WINDOW) \n",
    "  var newWin:Boolean = false\n",
    "  while( ! newWin) {\n",
    "      \n",
    "    // .dat files are little endian \n",
    "    val value = java.lang.Short.reverseBytes(input.readShort())\n",
    "    //print (\"%x \".format(value), value)\n",
    "    buffer( pos ) = (value.toDouble * scale) \n",
    "    available += 1\n",
    "    pos +=1\n",
    "    if ( pos == WINDOW )  // buffer ends\n",
    "      pos = 0  \n",
    "    if (available == WINDOW) {    \n",
    "      var j =0\n",
    "      for( i <- pos until WINDOW){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      for( i <- 0 until pos ){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      //dataWindow.foreach(print)\n",
    "      //println(\"|\")\n",
    "      newWin = true\n",
    "      available = WINDOW-STEP   // ready for read windows displacement offset \n",
    "    }\n",
    "  }\n",
    "  windows.append(dataWindow  )  \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399840"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nWindows*WINDOW*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining windowing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "var windowingF  =  new Array[Double](WINDOW)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until WINDOW) {\n",
    "    val y = Math.sin(Math.PI * i / (WINDOW - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ECGRdd = sc.parallelize(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// spark 2.1 mllib\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,1.012314263880639E-4,4.0464355541035393E-4,9.093907230442163E-4,0.0016140661084035513,0.0025167056548009467,0.003614793543925933,0.004905269207881844,0.006377707478939039,0.00807429448699254,0.009913595787027066,0.012013206129932157,0.014280366134827152,0.0167322054182821,0.019323571305776936,0.022101607320991853,0.025042562239781404,0.028169027604718466,0.03145115177018974,0.03491762007786862,0.038452935898706575,0.04215667424037047,0.045656810694352463,0.04952196991069394,0.05348414800125847,0.05753337213625115,0.061534544920459504,0.06545285705650494,0.06932339379951591,0.07334646968019955,0.07745668860932527,0.08140666562467924,0.08533442188010683,0.08922883342189634,0.09279395654519877,0.09667583177614536,0.100808408734863,0.104255..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ECGwnRdd = ECGRdd.map( x => ((x,windowingF).zipped.map(_*_))).map(x =>( x, Vectors.norm( Vectors.dense(x),2) )).map(x => Vectors.dense(x._1.map( c => c/ x._2)) ) \n",
    "ECGwnRdd.count\n",
    "ECGwnRdd.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ECGwnRdd.cache()  // important to kmeans time\n",
    "    val numIterations = 20// we want to assure a global mininum with 20 it seem not to be reached\n",
    "    val numClusters = 400\n",
    "    val clusters = KMeans.train(ECGwnRdd, numClusters, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@570a3627"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/fsainz/data/out/dict-105s1_W120.LEmodel SAVED\n"
     ]
    }
   ],
   "source": [
    "clusters.save(sc, outFileName)\n",
    "println(outFileName + \" SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
