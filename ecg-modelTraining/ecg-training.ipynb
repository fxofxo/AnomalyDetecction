{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.version\n",
    "val conf = sc.hadoopConfiguration\n",
    "val fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "val hdfshome = fs.getHomeDirectory\n",
    "println(sc.version)\n",
    "println(hdfshome)\n",
    "val files = fs.listFiles(hdfshome, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rootDir = \"/user/fsainz/data/in\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rootDir = \"hdfs://spark-clu-m/user/fsainz/data/out/\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.exists(new org.apache.hadoop.fs.Path(\"hdfs://spark-clu-m/user/fsainz/data/in/a02.dat\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io._                                                        \n",
    "import scala.collection.mutable.ListBuffer   //bE CAREFULL With java imports, better before scala.\n",
    "\n",
    "val WINDOW = 32;\n",
    "val STEP = 2;\n",
    "val TRsamples = 200000;\n",
    "val scale = 1/200.0;\n",
    "val inFileName = \"a02.dat\"\n",
    "\n",
    "val inputPath =  \"/home/fsainz/src/data/\" //local filesystem\n",
    "val fileName = inputPath + inFileName \n",
    "val hdfsOutPath = \"/user/fsainz/data/out/\"\n",
    "val outFileName = hdfsOutPath + \"dict-\" + inFileName.split('.')(0) + \"_W\" + WINDOW +\".model\"\n",
    "\n",
    "\n",
    "println(\"InFile: \" + fileName)\n",
    "println(\"Ouput Model: \" + outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "// Till Now start reading local file   TRaining file is a02.dat training and run\n",
    "\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "\n",
    "// val nSamples =  (inFile.length() / 2)   // We are going to read short: 2 bytes..\n",
    "\n",
    "\n",
    "val nWindows = TRsamples / WINDOW\n",
    "println(\"nWindows = \" + nWindows.toString)\n",
    "\n",
    "val buffer = new Array[Double](WINDOW)\n",
    "var windows = scala.collection.mutable.ListBuffer.empty[Array[Double]]\n",
    "\n",
    "var pos = 0\n",
    "var available = 0\n",
    "var dataWindow = new Array[Double](WINDOW)\n",
    "\n",
    "for (  w  <- 1 to nWindows.toInt){\n",
    "    // new object is neeed to append at the end!!\n",
    "  dataWindow = new Array[Double](WINDOW) \n",
    "  var newWin:Boolean = false\n",
    "  while( ! newWin) {\n",
    "    val value = input.readShort()\n",
    "    buffer( pos ) = (value.toDouble * scale) \n",
    "    available += 1\n",
    "    pos +=1\n",
    "    if ( pos == WINDOW )  // buffer ends\n",
    "      pos = 0  \n",
    "    if (available == WINDOW) {    \n",
    "      var j =0\n",
    "      for( i <- pos until WINDOW){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      for( i <- 0 until pos ){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      //dataWindow.foreach(print)\n",
    "      //println(\"|\")\n",
    "      newWin = true\n",
    "      available = WINDOW-STEP   // ready for read windows displacement offset \n",
    "    }\n",
    "  }\n",
    "  windows.append(dataWindow  )  \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nWindows*WINDOW*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining windowing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "var windowingF  =  new Array[Double](WINDOW)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until WINDOW) {\n",
    "    val y = Math.sin(Math.PI * i / (WINDOW - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ECGRdd = sc.parallelize(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// spark 2.1 mllib\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGwnRdd = ECGRdd.map( x => ((x,windowingF).zipped.map(_*_))).map(x =>( x, Vectors.norm( Vectors.dense(x),2) )).map(x => Vectors.dense(x._1.map( c => c/ x._2)) ) \n",
    "ECGwnRdd.count\n",
    "ECGwnRdd.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ECGwnRdd.cache()  // important to kmeans time\n",
    "    val numIterations = 20// we want to assure a global mininum with 20 it seem not to be reached\n",
    "    val numClusters = 400\n",
    "    val clusters = KMeans.train(ECGwnRdd, numClusters, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.save(sc, outFileName)\n",
    "println(outFileName + \" SAVED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Binay\n",
    "And improvement could be read file as binary file.\n",
    "/ TODO:\n",
    "//How to read a binary file from spark, that is partioned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hdfsinputPath =  \"/user/fsainz/data/in/\"\n",
    "val fileName = hdfsinputPath + inFileName\n",
    "val rawData = sc.binaryRecords(fileName,WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val n = rawData.count\n",
    "println(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData.first.getClass.getSimpleName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
