{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "hdfs://spark-clu-m/user/root\n"
     ]
    }
   ],
   "source": [
    "sc.version\n",
    "val conf = sc.hadoopConfiguration\n",
    "val fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "val hdfshome = fs.getHomeDirectory\n",
    "println(sc.version)\n",
    "println(hdfshome)\n",
    "val files = fs.listFiles(hdfshome, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rootDir = \"/user/fsainz/data/in\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocatedFileStatus{path=hdfs://spark-clu-m/user/fsainz/data/in/a01-tr.dat; isDirectory=false; length=5914000; replication=2; blocksize=134217728; modification_time=1511166838942; access_time=1511166838450; owner=fsainz; group=hadoop; permission=rw-r--r--; isSymlink=false}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocatedFileStatus{path=hdfs://spark-clu-m/user/fsainz/data/out/frames.csv; isDirectory=false; length=25075785; replication=2; blocksize=134217728; modification_time=1511266520221; access_time=1511266519968; owner=fsainz; group=hadoop; permission=rw-r--r--; isSymlink=false}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rootDir = \"hdfs://spark-clu-m/user/fsainz/data/out/\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)\n",
    "files.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: No more entry in hdfs://spark-clu-m/user/fsainz/data/out\n",
       "StackTrace:   at org.apache.hadoop.fs.FileSystem$6.next(FileSystem.java:1864)\n",
       "  at org.apache.hadoop.fs.FileSystem$6.next(FileSystem.java:1819)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.exists(new org.apache.hadoop.fs.Path(\"hdfs://spark-clu-m/user/fsainz/data/in/a02.dat\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nWindows = 99437\n"
     ]
    }
   ],
   "source": [
    "import java.io._                                                        \n",
    "import scala.collection.mutable.ListBuffer   //bE CAREFULL With java imports, better before scala.\n",
    "\n",
    "val WINDOW = 32;\n",
    "val STEP = 2;\n",
    "val SAMPLES = 200000;\n",
    "val scale = 1/200.0;\n",
    "\n",
    "\n",
    "// TODO:\n",
    "//How to read a binary file from spark, that is partioned...\n",
    "\n",
    "// Till Now start reading local file   TRaining file is a02.dat training and run\n",
    "val fileName = \"/home/fsainz/src/data/a02.dat\"\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "\n",
    "val nSamples =  (inFile.length() / 2)   // We are going to read short 2 bytes..\n",
    "val nWindows = nSamples / WINDOW\n",
    "println(\"nWindows = \" + nWindows.toString)\n",
    "\n",
    "val buffer = new Array[Double](WINDOW)\n",
    "var windows = scala.collection.mutable.ListBuffer.empty[Array[Double]]\n",
    "\n",
    "var pos = 0\n",
    "var available = 0\n",
    "var dataWindow = new Array[Double](WINDOW)\n",
    "\n",
    "for (  w  <- 1 to nWindows.toInt){\n",
    "    // new object is neeed to append at the end!!\n",
    "  dataWindow = new Array[Double](WINDOW) \n",
    "  var newWin:Boolean = false\n",
    "  while( ! newWin) {\n",
    "    val value = input.readShort()\n",
    "    buffer( pos ) = (value.toDouble * scale) \n",
    "    available += 1\n",
    "    pos +=1\n",
    "    if ( pos == WINDOW )  // buffer ends\n",
    "      pos = 0  \n",
    "    if (available == WINDOW) {    \n",
    "      var j =0\n",
    "      for( i <- pos until WINDOW){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      for( i <- 0 until pos ){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      //dataWindow.foreach(print)\n",
    "      //println(\"|\")\n",
    "      newWin = true\n",
    "      available = WINDOW-STEP   // ready for read windows displacement offset \n",
    "    }\n",
    "  }\n",
    "  windows.append(dataWindow  )  \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6363968"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nWindows*32*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining windowing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "var windowingF  =  new Array[Double](WINDOW)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until WINDOW) {\n",
    "    val y = Math.sin(Math.PI * i / (WINDOW - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// spark 2.1 mllib\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0,-0.0021907344132611356,-0.007950713201735033,-0.022378132364090354,-0.03328719680037869,-0.054610476643888156,-0.07567143918206366,-0.12864282231853222,-0.14054328799003904,-0.2007481057701149,-0.19267871337611953,-0.2587666949449433,-0.23526564020951385,-0.30086016942538835,-0.2788236154923073,-0.30242048126808463,-0.30242048126808463,-0.3310902829802477,-0.2674385146747268,-0.3136671032316897,-0.2443938865470467,-0.10278539018684428,0.011150252783065624,0.02810133954083573,0.06808937804316523,0.1803930734000773,0.11758723237710733,0.02495727348782054,0.0015979921576588937,-0.005060571093614103,-0.0029207415851386174,-1.0430565375392333E-32]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ECGwnRdd = ECGRdd.map( x => ((x,windowingF).zipped.map(_*_))).map(x =>( x, Vectors.norm( Vectors.dense(x),2) )).map(x => Vectors.dense(x._1.map( c => c/ x._2)) ) \n",
    "ECGwnRdd.count\n",
    "ECGwnRdd.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ECGwnRdd.cache()  // important to kmeans time\n",
    "    val numIterations = 20// we want to assure a global mininum with 20 it seem not to be reached\n",
    "    val numClusters = 400\n",
    "    val clusters = KMeans.train(ECGwnRdd, numClusters, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@381685cc"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters.save(sc, \"/user/fsainz/data/out/clusters-a02.model\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Try Binay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rootDir = \"/user/fsainz/DATA/in/\"\n",
    "val inFile = rootDir + \"a02.dat\"\n",
    "\n",
    "\n",
    "val rawData = sc.binaryRecords(inFile,WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val n = rawData.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byte[]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.first.getClass.getSimpleName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
