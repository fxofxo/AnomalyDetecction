{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val WINDOW = 32;\n",
    "val STEP = 2;\n",
    "val SAMPLES = 200000;\n",
    "val scale = 1/200.0;  // Mv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Databricks  \n",
    "//val rootDir = \"FileStore/fxo/ted/\"   //local File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// GCP\n",
    "val rootDir = \"hdfs://spark-clu-m/user/fsainz/data/out/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var windowingF  =  new Array[Double](WINDOW)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until WINDOW) {\n",
    "    val y = Math.sin(Math.PI * i / (WINDOW - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDataframe ( wSize: Int, frame : Array[Double] , model:KMeansModel) : Array[Double] = {\n",
    "  \n",
    "  // Split frame in WINDOW size windows\n",
    "  val frameLength = frame.length \n",
    "  \n",
    "  assert( frameLength % wSize == 0 )  // should throws a exception if not\n",
    "  \n",
    "  val signalWindows = scala.collection.mutable.ArrayBuffer.empty[Array[Double]]\n",
    "  val codecWindows = scala.collection.mutable.ArrayBuffer.empty[Array[Double]]\n",
    "  val ret = scala.collection.mutable.ArrayBuffer.empty[Double]\n",
    "  val nWindows:Int = frameLength / wSize \n",
    "  \n",
    "  for ( i <- 0 until (nWindows * 2)-1) {   // we step 1/2 wSize\n",
    "    var p = i*wSize/2\n",
    "    var signalWindow = frame.slice(p, p+wSize  )\n",
    "   \n",
    "    var tapWindow = (signalWindow,windowingF).zipped.map(_*_)   // multiple vector item by item\n",
    "    val wNorm = Vectors.norm(Vectors.dense(tapWindow) , 2)\n",
    "    tapWindow = tapWindow.map( x => x/wNorm)   \n",
    "   \n",
    "    val nc = model.predict(Vectors.dense(tapWindow))\n",
    "    \n",
    "    val cluster = model.clusterCenters(nc)\n",
    "   \n",
    "    val codecWindow = cluster.toArray.map(x => x*wNorm)   // scale to signal \n",
    " \n",
    "    signalWindows.append(signalWindow)\n",
    "    codecWindows.append(codecWindow.toArray)\n",
    "    // Coded windows has tapering function scale. get it of it usign a trick sum two desplacements halfs. \n",
    "    // see propierties or tapering funtion.\n",
    "    // we produce half a window a time\n",
    "    // and we have to arrange in some way first and last halfs\n",
    "    var half1 = new Array[Double](wSize)\n",
    "    var half2 = new Array[Double](wSize)\n",
    "    if (i == 0) {\n",
    "      half1 = codecWindows(i).slice(0,wSize/2)  \n",
    "      half2 = half1  \n",
    "    } \n",
    "    else {\n",
    "      print(\",\"+i+\",\")\n",
    "      half1= codecWindows(i-1).slice(wSize/2,wSize)\n",
    "      half2= codecWindows(i).slice(0,wSize/2)  \n",
    "      \n",
    "    }\n",
    "    var r = (half1 , half2).zipped.map(_+_)   // sum halfs\n",
    "    ret ++= r\n",
    "    // last half window \n",
    "    if ( i == (nWindows*2) -2 ) {\n",
    "      half1= codecWindows(i).slice(wSize/2,wSize)\n",
    "      half2= half1 \n",
    "      print (\"*************LAST + \" + i)\n",
    "      r = (half1 , half2).zipped.map(_+_)   // sum halfs\n",
    "      ret ++= r\n",
    "    }\n",
    "  }\n",
    "  ret.toArray\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "val csvFileName = rootDir + \"frames-a02.csv\"\n",
    "val framesECGRdd = sc.textFile(csvFileName).map(x => x.split(\",\").map(_.toDouble) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val clusters = KMeansModel.load(sc, rootDir + \"clusters-a02.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val framesCodedRdd = framesECGRdd.map( x => (x, processDataframe(WINDOW, x ,clusters ) )).map(x => (x._1, x._2, (x._1,x._2).zipped.map(_-_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "framesCodedRdd.first._2.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import sqlContext.implicits._\n",
    "framesCodedRdd.toDF.createOrReplaceTempView(\"framesProccesed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.0, -0.17033731469472463, -0.717587255486073, -1.6443707239788117, -3.008325681765548, -4.672696373233639, -6.937489663316098, -8.728333664575432, -11.735259518263517, -13.69173554456661, -17.052369410925444, -18.789221689858927, -21.753121458052835, -22.865286305072438, -25.085968785164773, -20.899083871184242, -3.3394402666518817, 4.4836301822035205, 6.5048418471739495, 13.870409202543007, 27.284385923510396, 20.58716473187961, 7.7384088268779205, 0.39773206602598077, -4.706234479185415, -13.022765772724476, -24.93486483477633, -29.88374242851996, -29.763253341886966, -26.658623825549466, -31.459493344629088, -20.005565988772084, 29.551227070244536, -52.670004318776904, 57.81694307060094, 72.64351910129903, -86.33007994630364, -75.12524819389554, -42.484..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framesCodedRdd.first._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import sqlContext.implicits._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val framesCodedDF = framesCodedRdd.toDF\n",
    "framesCodedDF.createOrReplaceTempView(\"framesProccesed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.UnsupportedOperationException\n",
       "Message: CSV data source does not support array<double> data type.\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.org$apache$spark$sql$execution$datasources$csv$CSVUtils$$verifyType$1(CSVUtils.scala:127)\n",
       "  at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)\n",
       "  at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:98)\n",
       "  at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.verifySchema(CSVUtils.scala:131)\n",
       "  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:65)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:142)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n",
       "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var outputFileName =\"/user/fsainz/data/out/framesCoded-a02.csv\"\n",
    "framesCodedDF.write.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").mode(\"overwrite\").save(outputFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "framesCodedDF.createOrReplaceTempView(\"framesProccesed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
