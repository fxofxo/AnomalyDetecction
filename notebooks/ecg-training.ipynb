{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Train the model. Gets kmeans dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.version\n",
    "val conf = sc.hadoopConfiguration\n",
    "val fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "val hdfshome = fs.getHomeDirectory\n",
    "println(sc.version)\n",
    "println(hdfshome)\n",
    "val files = fs.listFiles(hdfshome, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rootDir = \"/user/fsainz/data/out\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//fs.exists(new org.apache.hadoop.fs.Path(\"hdfs://spark-clu-m/user/fsainz/data/in/a02.dat\"))\n",
    "fs.exists(new org.apache.hadoop.fs.Path(rootDir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing. Split in Signal in windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//val inFileName = \"105s1\"   //mit-arritmia\n",
    "//val wSize = 120;              // sliding windows size   \n",
    "//val step = wSize/10;          // window overriding step  (10% of windows size)        \n",
    "\n",
    "val inFileName = \"105\"   //mit-arritmia\n",
    "val wSize = 120;              // sliding windows size   \n",
    "val step = wSize/10;          // window overriding step  (10% of windows size)    \n",
    "\n",
    "//val inFileName = \"a02\"     //apnea\n",
    "//val wSize = 40;              // sliding windows size   \n",
    "//val step = wSize/10;   \n",
    "\n",
    "val scale = 1/200.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import java.io._                                                        \n",
    "import scala.collection.mutable.ListBuffer   //bE CAREFULL With java imports, better before scala.\n",
    "\n",
    "\n",
    "val inputPath =  \"/home/fsainz/notebooks/data/\" //local filesystem\n",
    "val fileName = inputPath + inFileName + \".dat\"\n",
    "val hdfsOutPath = \"/user/fsainz/data/out/\"\n",
    "val outFileName = hdfsOutPath + \"dict-\" + inFileName + \"_W\" + wSize + \"-\"+step +\".LE.model\"\n",
    "\n",
    "\n",
    "println(\"InFile: \" + fileName)\n",
    "println(\"Ouput Model: \" + outFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a circular buffer to read data from file, and obtain sliding windows\n",
    "Take into account that arithmia db file has two signal, and we use only one of then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "val TRsamples = (maxSlidingW * .2).toInt;       // number of samples to train the model## Read Apnea DB file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Apnea DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "// Now start reading local file   TRaining file is part InFile training and run\n",
    "\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "val maxSlidingW= (inFile.length / 2) / step    // each data in the file is two byte long\n",
    "val TRsamples = (maxSlidingW * .2).toInt;       // number of samples to train the model\n",
    "\n",
    "// val nSamples =  (inFile.length() / 2)   // We are going to read short: 2 bytes..\n",
    "\n",
    "\n",
    "val buffer = new Array[Double](wSize)\n",
    "var windows = scala.collection.mutable.ListBuffer.empty[Array[Double]]\n",
    "\n",
    "var p_buff = 0\n",
    "var available = 0\n",
    "var dataWindow = new Array[Double](wSize)\n",
    "var readed = 0\n",
    "\n",
    "// Read data and create slinding windows. Every STEPS reads create a new windows\n",
    "for (  w  <- 1 to TRsamples){\n",
    "  dataWindow = new Array[Double](wSize)  // new object is need to append to the list!!\n",
    "  var newWin:Boolean = false\n",
    "  while( ! newWin) { \n",
    "    // .dat files are little endian \n",
    "    val value = java.lang.Short.reverseBytes(input.readShort())\n",
    "    \n",
    "    print (\"%x \".format(value), value, \"\\n\")\n",
    "    buffer( p_buff ) = (value.toDouble * scale) \n",
    "    available += 1\n",
    "    p_buff +=1\n",
    "    if ( p_buff == wSize )  // buffer ends\n",
    "      p_buff = 0  \n",
    "      \n",
    "    if (available == wSize) {    // There is a window ready\n",
    "      var j = 0\n",
    "      // buffer is circular buffers last readings firts.\n",
    "      for( i <- p_buff until wSize){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      for( i <- 0 until p_buff ){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      //dataWindow.foreach(print)\n",
    "      //println(\"|\")\n",
    "      newWin = true\n",
    "      available = wSize-step   // ready for read wSize displacement offset \n",
    "    }\n",
    "  }\n",
    "  windows.append(dataWindow  )  \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Arithmia db files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "// Now start reading local file   TRaining file is part InFile training and run\n",
    "\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "val fileSamples = inFile.length / 3     // arithmia files records have two signal of 12 bytes length 3bytes\n",
    "val maxSlidingW= fileSamples / step    // each data in the file is two byte long\n",
    "val TRsamples = (maxSlidingW * .2).toInt;       // number of samples to train the model\n",
    "\n",
    "val u4mask = 0xf0\n",
    "val l4mask = 0x0f\n",
    "\n",
    "val buffer = new Array[Double](wSize)\n",
    "var windows = scala.collection.mutable.ListBuffer.empty[Array[Double]]\n",
    "\n",
    "var p_buff = 0\n",
    "var available = 0\n",
    "var dataWindow = new Array[Double](wSize)\n",
    "var readed = 0\n",
    "val r = new Array[Byte](3)\n",
    "\n",
    "// Read data and create slinding windows. Every STEPS reads create a new windows\n",
    "for (  w  <- 1 to TRsamples){\n",
    "  dataWindow = new Array[Double](wSize)  // new object is need to append to the list!!\n",
    "  var newWin:Boolean = false\n",
    "  while( ! newWin) { \n",
    "    // .dat files are little endian \n",
    "    input.read(r)\n",
    "    val s1 = ( ((r(1) & l4mask)) << 8 ) + (r(0) & 0xff)\n",
    "    val s2 = ( (r(1) & u4mask )  << 4 ) + (r(2) & 0xff)  \n",
    "      \n",
    "    print (\"%x \".format(s1), \"%x\".format(s2), \"\\n\")\n",
    "      \n",
    "    buffer( p_buff ) = (s1.toDouble * scale) \n",
    "    available += 1\n",
    "    p_buff +=1\n",
    "    if ( p_buff == wSize )  // buffer ends\n",
    "      p_buff = 0  \n",
    "      \n",
    "    if (available == wSize) {    // There is a window ready\n",
    "      var j = 0\n",
    "      // buffer is circular buffers last readings firts.\n",
    "      for( i <- p_buff until wSize){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      for( i <- 0 until p_buff ){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      //dataWindow.foreach(print)\n",
    "      //println(\"|\")\n",
    "      newWin = true\n",
    "      available = wSize-step   // ready for read wSize displacement offset \n",
    "    }\n",
    "  }\n",
    "  windows.append(dataWindow  )  \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining windowing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var windowingF  =  new Array[Double](wSize)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until wSize) {\n",
    "    val y = Math.sin(Math.PI * i / (wSize - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// spark 2.1 mllib\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and Apply Windowing function each segment before clustering\n",
    "Nomalizar $$ (X-mean(x))/Norm(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following should work with no problem but it not!!. but It works in spark-shell.Â¶\n",
    "So it seems to be a problems of Apache Toree jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGRdd = sc.parallelize(windows)\n",
    "val ECGnormRdd = ECGRdd.map( x => { val n= x.length; val s = x.reduce(_+_); (x,s/n) }).   // get avg\n",
    "                        map( x => (Vectors.dense(x._1), x._1.map(i => i - x._2))).                    // susbtracs avg\n",
    "                        map( x => { val nr = Vectors.norm(x._1,2); x._2.map( i => i/nr) })\n",
    "\n",
    "val ECGwnRdd = ECGnormRdd.map( x => ((x,windowingF).zipped.map(_*_)))\n",
    "ECGwnRdd.first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So make a workaround of save windows to files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// FileWriter\n",
    "import org.apache.hadoop.fs.Path;\n",
    "val csvFileName = hdfsOutPath + \"wslices-f\"+ inFileName + \"-w\" +wSize +\"-s\"+step + \".csv\"\n",
    "\n",
    "// write on hdfs as normal file\n",
    "print(\"SAving: \" +csvFileName )\n",
    "val output = fs.create(new Path(csvFileName))\n",
    "\n",
    "println(\"writing windows files: \"+ csvFileName)\n",
    "//val bw = new BufferedWriter(new FileWriter(output))\n",
    "for ( w <- windows ){\n",
    "    var line = \"\"\n",
    "   w.foreach( x => line +=  x.toString + \",\" )\n",
    "   output.write( (line.substring(0,line.length-1) + \"\\n\").getBytes())\n",
    "} \n",
    "output.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And read in spark...\n",
    "It seem to be necesary to restart the jupyter kernel before progress.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvFileName = hdfsOutPath + \"wslices-f\"+ inFileName + \"-w\" +wSize +\"-s\"+step + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading windows \" + csvFileName )\n",
    "val ECGRdd = sc.textFile(csvFileName).map(x => x.split(\",\").map(_.toDouble) )\n",
    "ECGRdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGnormRdd = ECGRdd.map( x => { val n= x.length; val s = x.reduce(_+_); (x,s/n) }).   // get avg\n",
    "                        map( x => (Vectors.dense(x._1), x._1.map(i => i - x._2))).                    // susbtracs avg\n",
    "                        map( x => { val nr = Vectors.norm(x._1,2); x._2.map( i => i/nr) })\n",
    "\n",
    "ECGnormRdd.first.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGwnRdd = ECGnormRdd.map( x => Vectors.dense( ((x,windowingF).zipped.map(_*_)) ))\n",
    "ECGwnRdd.first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.fs.Path;\n",
    "val tapWin = ECGnormRdd.collect()\n",
    "// write on hdfs as normal fileECGwnRdd\n",
    "val csvtapWindows = hdfsOutPath + \"TAPslices-\"+ inFileName+ wSize+\"-\"+step + \".csv\"\n",
    "print(\"SAving: \" +csvtapWindows )\n",
    "val output = fs.create(new Path(csvtapWindows))\n",
    "\n",
    "println(\"writing windows files: \"+ csvtapWindows)\n",
    "//val bw = new BufferedWriter(new FileWriter(output))\n",
    "for ( w <- tapWin ){\n",
    "    var line = \"\"\n",
    "   w.toArray.foreach( x => line +=  x.toString + \",\" )\n",
    "   output.write( (line.substring(0,line.length-1) + \"\\n\").getBytes())\n",
    "} \n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ECGwnRdd.cache()  // important to kmeans time\n",
    "    val numIterations = 20// we want to assure a global mininum with 20 it seem not to be reached\n",
    "    val numClusters = ( ECGRdd.count * 0.10).toInt\n",
    "    val clusters = KMeans.train(ECGwnRdd, numClusters, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.save(sc, outFileName)\n",
    "\n",
    "println(outFileName + \" SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
