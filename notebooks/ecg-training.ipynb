{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Train the model. Gets kmeans dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.version\n",
    "val conf = sc.hadoopConfiguration\n",
    "val fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "val hdfshome = fs.getHomeDirectory\n",
    "println(sc.version)\n",
    "println(hdfshome)\n",
    "val files = fs.listFiles(hdfshome, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rootDir = \"/user/fsainz/data/out\"\n",
    "val files = fs.listFiles(new org.apache.hadoop.fs.Path(rootDir), false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//fs.exists(new org.apache.hadoop.fs.Path(\"hdfs://spark-clu-m/user/fsainz/data/in/a02.dat\"))\n",
    "fs.exists(new org.apache.hadoop.fs.Path(rootDir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io._                                                        \n",
    "import scala.collection.mutable.ListBuffer   //bE CAREFULL With java imports, better before scala.\n",
    "\n",
    "val WINDOW = 120;\n",
    "val STEP = 4;\n",
    "val TRsamples = 200000;\n",
    "val scale = 1/200.0;\n",
    "//val inFileName = \"a02.dat\"     //apnea\n",
    "val inFileName = \"105s1.dat\"   //mit-arritmia\n",
    "\n",
    "val inputPath =  \"/home/fsainz/data/\" //local filesystem\n",
    "val fileName = inputPath + inFileName \n",
    "val hdfsOutPath = \"/user/fsainz/data/out/\"\n",
    "val outFileName = hdfsOutPath + \"dict-\" + inFileName.split('.')(0) + \"_W\" + WINDOW +\".LEmodel\"\n",
    "\n",
    "\n",
    "println(\"InFile: \" + fileName)\n",
    "println(\"Ouput Model: \" + outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "// Now start reading local file   TRaining file is part InFile training and run\n",
    "\n",
    "var inFile =  new File(fileName)\n",
    "var InFileStream = new FileInputStream( inFile )\n",
    "val input : DataInputStream  = new DataInputStream(InFileStream)\n",
    "\n",
    "\n",
    "// val nSamples =  (inFile.length() / 2)   // We are going to read short: 2 bytes..\n",
    "\n",
    "\n",
    "val nWindows = TRsamples / WINDOW\n",
    "println(\"nWindows = \" + nWindows.toString)\n",
    "\n",
    "val buffer = new Array[Double](WINDOW)\n",
    "var windows = scala.collection.mutable.ListBuffer.empty[Array[Double]]\n",
    "\n",
    "var pos = 0\n",
    "var available = 0\n",
    "var dataWindow = new Array[Double](WINDOW)\n",
    "\n",
    "for (  w  <- 1 to nWindows.toInt){\n",
    "    // new object is neeed to append at the end!!\n",
    "  dataWindow = new Array[Double](WINDOW) \n",
    "  var newWin:Boolean = false\n",
    "  while( ! newWin) {\n",
    "      \n",
    "    // .dat files are little endian \n",
    "    val value = java.lang.Short.reverseBytes(input.readShort())\n",
    "    //print (\"%x \".format(value), value)\n",
    "    buffer( pos ) = (value.toDouble * scale) \n",
    "   \n",
    "    available += 1\n",
    "    pos +=1\n",
    "    if ( pos == WINDOW )  // buffer ends\n",
    "      pos = 0  \n",
    "    if (available == WINDOW) {    \n",
    "      var j =0\n",
    "      for( i <- pos until WINDOW){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      for( i <- 0 until pos ){\n",
    "         dataWindow(j) = buffer(i)\n",
    "         j +=1\n",
    "      }\n",
    "      //dataWindow.foreach(print)\n",
    "      //println(\"|\")\n",
    "      newWin = true\n",
    "      available = WINDOW-STEP   // ready for read windows displacement offset \n",
    "    }\n",
    "  }\n",
    "  windows.append(dataWindow  )  \n",
    "}\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nWindows*WINDOW*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining windowing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var windowingF  =  new Array[Double](WINDOW)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until WINDOW) {\n",
    "    val y = Math.sin(Math.PI * i / (WINDOW - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following should work with no problem but it not!!. It works in spark-shell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGRdd = sc.parallelize(windows)\n",
    "ECGRdd.map(x => (x,windowingF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So make workoarround of save windows to files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// FileWriter\n",
    "import org.apache.hadoop.fs.Path;\n",
    "val csvFileName = hdfsOutPath + \"wslices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val output = fs.create(new Path(csvFileName))\n",
    "\n",
    "\n",
    "val bw = new BufferedWriter(new FileWriter(file))\n",
    "for ( w <- windows){\n",
    "    var line = \"\"\n",
    "   w.foreach( x => line +=  x.toString + \",\" )\n",
    "   output.write( (line.substring(1,line.length-1) + \"\\n\").getBytes())\n",
    "} \n",
    "bw.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGRdd = sc.textFile(csvFileName).map(x => x.split(\",\").map(_.toDouble) )\n",
    "ECGRdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECGRdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark 2.1 mllib\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ECGwnRdd = ECGRdd.map( x => ((x,windowingF).zipped.map(_*_))).map(x =>( x, Vectors.norm( Vectors.dense(x),2) )).map(x => Vectors.dense(x._1.map( c => c/ x._2)) ) \n",
    "ECGwnRdd.count\n",
    "ECGwnRdd.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ECGwnRdd.cache()  // important to kmeans time\n",
    "    val numIterations = 20// we want to assure a global mininum with 20 it seem not to be reached\n",
    "    val numClusters = 400\n",
    "    val clusters = KMeans.train(ECGwnRdd, numClusters, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.save(sc, outFileName)\n",
    "println(outFileName + \" SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
