{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Coded Frames using k-means dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val scale = 1/200.0\n",
    "val samplesPerSecond = 360\n",
    "\n",
    "val WINDOW = 120 // 32 or 40 samples per window\n",
    "val WindowsPerFrame = 30 // 30 ,32 or 150\n",
    "//val STEP = 2\n",
    "//val SAMPLES = 200000\n",
    "\n",
    "val samplesPerFrame = WindowsPerFrame * WINDOW\n",
    "val secondPerFrame = samplesPerFrame / samplesPerSecond\n",
    "val inputDataFile = \"105s1.dat\"\n",
    "//val inputDataFile = \"a02.dat\"\n",
    "val hdfsOutPath = \"/user/fsainz/data/out/\"\n",
    "\n",
    "\n",
    "val framesFileName = \"frames-\"+ inputDataFile.split('.')(0)+\"_\" + secondPerFrame +\"sLE.csv\"\n",
    "val ModelFileName = hdfsOutPath + \"dict-\" + inputDataFile.split('.')(0) + \"_W\" + WINDOW +\".LEmodel\"\n",
    "\n",
    "var outputPath = \"/user/fsainz/data/out/framesCoded-\"+ inputDataFile.split('.')(0) + \"_\" +secondPerFrame  +\"sLE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Databricks  \n",
    "//val rootDir = \"FileStore/fxo/ted/\"   //local File\n",
    "// GCP\n",
    "val rootDir = \"hdfs://spark-clu-m/user/fsainz/data/out/\" \n",
    "val csvFileName = rootDir + framesFileName\n",
    "println(\"Procesing: \" + csvFileName)\n",
    "println(\"With: \" + ModelFileName)\n",
    "println(\"Output Files: \" + outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var windowingF  =  new Array[Double](WINDOW)   //differs of Array[Double](WINDOW) uajj\n",
    "for ( i <- 0 until WINDOW) {\n",
    "    val y = Math.sin(Math.PI * i / (WINDOW - 1.0));\n",
    "          windowingF(i)=  y * y\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Till now there is a bug in proccesDataFrame function, processing contour of the frames (firts and last half window) so coded data is trunked!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataframe ( wSize: Int, frame : Array[Double] , model:KMeansModel) : Array[Double] = {\n",
    "  \n",
    "  // Split frame in WINDOW size windows\n",
    "  val frameLength = frame.length \n",
    "  \n",
    "  assert( frameLength % wSize == 0 )  // should throws a exception if not\n",
    "  \n",
    "  val signalWindows = scala.collection.mutable.ArrayBuffer.empty[Array[Double]]\n",
    "  val codecWindows = scala.collection.mutable.ArrayBuffer.empty[Array[Double]]\n",
    "  val ret = scala.collection.mutable.ArrayBuffer.empty[Double]\n",
    "  val nWindows:Int = frameLength / wSize \n",
    "  \n",
    "  for ( i <- 0 until (nWindows * 2)-1) {   // we step 1/2 wSize\n",
    "    var p = i*wSize/2\n",
    "    var signalWindow = frame.slice(p, p+wSize  )\n",
    "   \n",
    "    var tapWindow = (signalWindow,windowingF).zipped.map(_*_)   // multiple vector item by item\n",
    "    val wNorm = Vectors.norm(Vectors.dense(tapWindow) , 2)\n",
    "    tapWindow = tapWindow.map( x => x/wNorm)   \n",
    "   \n",
    "    val nc = model.predict(Vectors.dense(tapWindow))\n",
    "    \n",
    "    val cluster = model.clusterCenters(nc)\n",
    "   \n",
    "    val codecWindow = cluster.toArray.map(x => x*wNorm)   // scale to signal \n",
    " \n",
    "    signalWindows.append(signalWindow)\n",
    "    codecWindows.append(codecWindow.toArray)\n",
    "    // Coded windows has tapering function scale. get it of it usign a trick sum two desplacements halfs. \n",
    "    // see propierties or tapering funtion.\n",
    "    // we produce half a window a time\n",
    "    // and we have to arrange in some way first and last halfs\n",
    "    var half1 = new Array[Double](wSize)\n",
    "    var half2 = new Array[Double](wSize)\n",
    "    if (i == 0) {\n",
    "      half1 = codecWindows(i).slice(0,wSize/2)  \n",
    "      half2 = half1  \n",
    "      print(\"+\")\n",
    "      val r = (half1 , half2).zipped.map(_+_)   // sum halfs\n",
    "     // ret ++= r\n",
    "    } \n",
    "    else {\n",
    "      print(\"*\")\n",
    "      half1= codecWindows(i-1).slice(wSize/2,wSize)\n",
    "      half2= codecWindows(i).slice(0,wSize/2)  \n",
    "      val r = (half1 , half2).zipped.map(_+_)   // sum halfs\n",
    "      ret ++= r\n",
    "    }\n",
    "    \n",
    "    // last half window \n",
    "    if ( i == (nWindows*2) -2 ) {\n",
    "      half1= codecWindows(i).slice(wSize/2,wSize)\n",
    "      half2= half1 \n",
    "      print (\"-\" )\n",
    "      val r = (half1 , half2).zipped.map(_+_)   // sum halfs\n",
    "      // ret ++= r\n",
    "    }\n",
    "  }\n",
    "  ret.toArray\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val framesECGRdd = sc.textFile(csvFileName).map(x => x.split(\",\").map(_.toDouble) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val clusters = KMeansModel.load(sc, ModelFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val halfWindowsSize = WINDOW /2 \n",
    "\n",
    "val top = samplesPerFrame-halfWindowsSize\n",
    "top-halfWindowsSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framesECGRdd.first().slice(halfWindowsSize,samplesPerFrame-halfWindowsSize).length\n",
    "framesECGRdd.first().slice(halfWindowsSize,top).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val framesCodedRdd = framesECGRdd.map( x => (x, processDataframe(WINDOW, x ,clusters ) )).map(x => (x._1.slice(halfWindowsSize,top ), x._2, (x._1.slice(halfWindowsSize,samplesPerFrame-halfWindowsSize),x._2).zipped.map(_-_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framesCodedRdd.first._2.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "framesCodedRdd.map(x => (x._1.mkString(\",\") + \";\" + x._2.mkString(\",\") + \";\" + x._3.mkString(\",\"))).saveAsTextFile(outputPath)\n",
    "println(\" SAVED: \" + outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// It no posible to save directly as csv if we want to put several field data...//\n",
    "///var outputFileName =\"/user/fsainz/data/out/framesCoded-a02.csv\"//\n",
    "//framesCodedDF.write.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").mode(\"overwrite\").save(outputFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// it a pity, It is not possible to share tables with pySpark..\n",
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlContext = new SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "framesCodedRdd.toDF.createOrReplaceTempView(\"framesProccesed\")\n",
    "framesCodedDF.createOrReplaceTempView(\"framesProccesed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
